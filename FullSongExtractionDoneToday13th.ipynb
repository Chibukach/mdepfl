{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os,IPython, librosa, mir_eval\n",
    "from sys import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join,isdir\n",
    "from IPython.display import Audio\n",
    "from librosa.display import waveplot,specshow\n",
    "from librosa.onset import onset_strength, onset_detect\n",
    "from librosa.feature import melspectrogram, mfcc\n",
    "from librosa import load\n",
    "\n",
    "from collections import defaultdict,OrderedDict\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "import scipy\n",
    "from pandas import HDFStore,DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Extraction from fma_small\n",
    "\n",
    "1. Find the song folder path relative to the current computer\n",
    "2. Retrieve the different genre classifications\n",
    "3. Identify each song via its full path to song using index, to guarantee one-to-one mapping \n",
    "4. Sort in alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieving path to the fma_small directory and the corresponding meta data\n",
    "HOME_DIR = IPython.utils.path.get_home_dir()\n",
    "\n",
    "temp = join(HOME_DIR, 'Documents')\n",
    "path_to_small_fma = join(temp, 'fma_small')\n",
    "json_file = join(path_to_small_fma,'fma_small.json')\n",
    "#locate meta_dta\n",
    "print(path_to_small_fma)\n",
    "df = pd.read_json(json_file)\n",
    "print(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#only choose top genre as the label\n",
    "df = df.loc[:,['top_genre']]\n",
    "\n",
    "#ensure that the genre name matches file name in fma_small, \n",
    "#i.e Oldtime / Historian conflict issue\n",
    "df['top_genre']=df['top_genre'].apply(lambda y: y.split(os.sep)[0].strip())\n",
    "#locate each individual song by its full path\n",
    "df['temp'] = path_to_small_fma\n",
    "str_index = [\"%.2d\" % x for x in df.index]\n",
    "complete_genre_list = df['top_genre'].unique()\n",
    "\n",
    "#create full path to file and store as a single array\n",
    "df['full_path_to_song'] = df.temp.map(str)+ \"/\"+ df['top_genre'].values+ \"/\"+ str_index+ \".mp3\"\n",
    "del df['temp']\n",
    "#keep songs according to alphabetical order of songs \n",
    "df.sort_values(by = 'top_genre', inplace = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "complete_genre_list_df = pd.DataFrame(complete_genre_list, columns = ['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieve number of songs per genre\n",
    "genre_and_count = df['top_genre'].value_counts().sort_index()\n",
    "all_songs_path = df['full_path_to_song'].values  \n",
    "print(genre_and_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_counts = [] \n",
    "ordered_genres = [] \n",
    "full_song_df = OrderedDict()\n",
    "genre_to_song_dict = {}\n",
    "\n",
    "\n",
    "#retrieve number of songs per genre with the order preserved in two lists\n",
    "#ordered_genres\n",
    "#song_counts\n",
    "for i,genre in enumerate(genre_and_count.index):\n",
    "    ordered_genres.append(genre)\n",
    "    temp = df['top_genre'].value_counts()[genre]\n",
    "    song_counts.append(temp)\n",
    "\n",
    "genre_to_song_zipped = zip(ordered_genres,song_counts)\n",
    "\n",
    "num_of_genre = np.shape(genre_and_count)[0]\n",
    "print(\"The number of genres is:\", num_of_genre)\n",
    "\n",
    "#dictionary with each song count and its corresponding genre\n",
    "for genre,song_count in genre_to_song_zipped:\n",
    "    genre_to_song_dict[genre] = song_count\n",
    "\n",
    "genre_to_song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_genre = np.shape(genre_and_count)[0]\n",
    "all_songs_path = df['full_path_to_song'].values\n",
    "\n",
    "paths_dict = OrderedDict()\n",
    "\n",
    "#prepend zero so we have a start point for all_songs_path \n",
    "#and avoid messing with indices\n",
    "\n",
    "#use the cumulative sum to find none uniform ranges\n",
    "song_counts.insert(0,0)\n",
    "cumulative_sum = np.cumsum(song_counts,dtype=int)\n",
    "\n",
    "#creates a dictionary of the genres and its corresponding path\n",
    "for i,genre in enumerate(ordered_genres):\n",
    "    str1=genre\n",
    "    str2 = \"_paths\"\n",
    "    genre_paths = \"\".join((str1,str2))\n",
    "    paths_dict[genre_paths] = all_songs_path[cumulative_sum[i]:cumulative_sum[i+1]]\n",
    "\n",
    "#paths_dict\n",
    "#{genre_path_name: genre_paths}\n",
    "print(\"{'Electronic_paths:[array_of_all_electronic_paths]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_of_songs = 3\n",
    "sampling_rate = 44100\n",
    "\n",
    "genre_signals_dict = OrderedDict()\n",
    "#creates a dictionary of the signals in a genre and their raw file\n",
    "for genre_path_name,genre_paths in paths_dict.items():\n",
    "    str1=genre_path_name[:-5]\n",
    "    str2 = \"signals\"\n",
    "    genre_signals = \"\".join((str1,str2))       \n",
    "    try:\n",
    "        first_three = genre_paths[:num_of_songs]\n",
    "        genre_signals_dict[genre_signals] = [\n",
    "        load(p,sr=None)[0] for p in first_three]\n",
    "    except IOError as exc:\n",
    "        print(\"Unable to locate folder\")\n",
    "        #raise IOError(\"%s: %s\" % (genre_paths, exc.strerror))\n",
    "        \n",
    "#genre_signals_dict\n",
    "#{genre_signals_name:genre_signals_paths}\n",
    "print(\"{'Electronic_signals:[array_of_all_electronic_paths]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the time series for each song according to the genres\n",
    "\n",
    "#sig_lengths = []\n",
    "for genre_signal_name,genre_signals in genre_signals_dict.items(): \n",
    "    for i, sig_amp in enumerate(genre_signals):\n",
    "        plt.subplot(1, num_of_songs, i+1)\n",
    "#        sig_lengths.append(len(sig_amp))\n",
    "        waveplot(sig_amp)\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.title(genre_signal_name)\n",
    "    plt.figure()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the complete feature extraction on a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#home laptop\n",
    "if \"Ubuntu\" in os.uname().version:\n",
    "    song, sampling_rate = librosa.load(\"/home/chib/Documents/fma_small/Electronic/99289.mp3\")\n",
    "\n",
    "#lab macbook\n",
    "elif \"Darwin\" in os.uname().version:\n",
    "    song, sampling_rate = librosa.load(\"/Users/chibmac/Documents/fma_small/Electronic/99289.mp3\")\n",
    "\n",
    "else: \n",
    "    song, sampling_rate = librosa.load(librosa.util.example_audio_file())\n",
    "    \n",
    "song.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_length = len(song)\n",
    "#number of chunks to split the song into\n",
    "num_of_hops_per_section = 3\n",
    "num_of_sections = 7\n",
    "num_of_features = 5\n",
    "\n",
    "tot_num_of_hops = num_of_hops_per_section*num_of_sections\n",
    "\n",
    "tot_num_of_hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hop_width = int(np.floor(song_length/tot_num_of_hops))\n",
    "hop_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "section_width = hop_width*num_of_hops_per_section\n",
    "section_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make song a multiple of the hops\n",
    "song = song[:(hop_width*tot_num_of_hops)]\n",
    "\n",
    "song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sps - section per song\n",
    "section_per_song = np.reshape(song,(1,num_of_sections,section_width))\n",
    "sps_song,sps_sect,sps_width =  section_per_song.shape\n",
    "\n",
    "print(\"Song number: %d, Number of sections:%d, Song Width: %d\" %(sps_song,sps_sect,sps_width) )\n",
    "\n",
    "section_per_song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ons - one song section\n",
    "one_song_section = section_per_song[0,0,:]\n",
    "one_song_section = np.reshape(one_song_section,(1,1,one_song_section.shape[0]))\n",
    "ons_song,ons_sect,ons_sect_width = one_song_section.shape\n",
    "\n",
    "\n",
    "print(\"Song number: %d, Section Number:%d, Section Width: %d\" %(ons_sect,ons_song,ons_sect_width) )\n",
    "\n",
    "one_song_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hps - hops per section\n",
    "hop_per_section = np.reshape(one_song_section,(1,1,num_of_hops_per_section,hop_width))\n",
    "hps_song, hps_sect,hps_hops,hps_hop_width = hop_per_section.shape\n",
    "\n",
    "print(\"Song number: %d, Section Number:%d, Number of hops: %d, Hop Width:%d\" %(hps_song,hps_sect,hps_hops,hps_hop_width))                                                                                                          \n",
    "hop_per_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_mfcc = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_song_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hop_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "section_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int(section_width/num_of_hops_per_section) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_song_section_squeezed = np.squeeze(one_song_section)\n",
    "complete_n_fft = hop_width+1\n",
    "mfcc_per_section = librosa.feature.mfcc(y=one_song_section_squeezed, sr=sampling_rate,\n",
    "                                    n_fft = complete_n_fft , n_mfcc=num_of_mfcc,hop_length = hop_width).T\n",
    "\n",
    "mfcc_per_section = np.reshape(mfcc_per_section,(1,1,mfcc_per_section.shape[0],mfcc_per_section.shape[1]))\n",
    "mps_song,mps_sect,mps_hops,mps_mfccs = mfcc_per_section.shape\n",
    "\n",
    "print(\"Song number: %d, Section Number:%d, Number of hops: %d, Number of mfccs:%d\" %(mps_song,mps_sect,mps_hops, mps_mfccs))  \n",
    "mfcc_per_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hop_per_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zps - zero crossing rate per section\n",
    "zcr_per_section = np.sum(librosa.core.zero_crossings(np.squeeze(hop_per_section)),axis =1 )\n",
    "\n",
    "zcr_per_section= np.reshape(zcr_per_section,(1,1,len(zcr_per_section),1))\n",
    "zps_song,zps_sect,zps_hops,zps_per_hop= zcr_per_section.shape\n",
    "print(\"Song number: %d, Section Number:%d, Number of hops: %d, ZCR per hop:%d\" %(zps_song,zps_sect,zps_hops,zps_per_hop)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onset_frames = librosa.onset.onset_detect(y=one_song_section_squeezed, sr=sampling_rate,units = 'samples')\n",
    "onset_length = len(onset_frames)\n",
    "onset_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_splitter = np.arange(0,len(one_song_section_squeezed)+1, hop_width,dtype=int)\n",
    "split_length = len(song_splitter)\n",
    "song_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_onsets = [0]\n",
    "for i in range(1,len(song_splitter)):\n",
    "    total_onsets.append(np.sum(onset_frames<song_splitter[i]))\n",
    "\n",
    "total_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onsets_per_section = np.reshape(np.diff(total_onsets),(one_song_section.shape[0],one_song_section.shape[1],len(np.diff(total_onsets)),1))\n",
    "ops_song,ops_sect,ops_hops,ops_per_hop = onsets_per_section.shape\n",
    "print(\"Song number: %d, Section Number:%d, Number of hops: %d, Number of Onsets per hop:%d\" %(ops_song,ops_sect,ops_hops,ops_per_hop)) \n",
    "\n",
    "onsets_per_section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_song_section_squeezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hop_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "librosa.stft(y=one_song_section_squeezed,  hop_length= hop_width,n_fft=complete_n_fft).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spectral_centroid_per_section = librosa.feature.spectral_centroid(y=one_song_section_squeezed, sr=sampling_rate, hop_length= hop_width,n_fft=complete_n_fft).T\n",
    "spectral_centroid_per_section = np.reshape(spectral_centroid_per_section,(1,1,spectral_centroid_per_section.shape[0],spectral_centroid_per_section.shape[1]))\n",
    "scps_song,scps_sect,scps_hops, scps_per_hop =  spectral_centroid_per_section.shape\n",
    "print(\"Song number: %d, Section Number:%d, Number of hops: %d, Number of spectral centroids per hop:%d\" %(scps_song,scps_sect,scps_hops,scps_per_hop)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oenv = librosa.onset.onset_strength(y=one_song_section_squeezed, sr=sampling_rate, hop_length=hop_width, n_fft =complete_n_fft)\n",
    "tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sampling_rate,\n",
    "                              hop_length=hop_width)\n",
    "ac_global_temp = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n",
    "ac_global = np.reshape(ac_global_temp, (one_song_section.shape[0],one_song_section.shape[1],len(ac_global_temp),1))\n",
    "ac_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"MFCC Per section: \",mfcc_per_section.shape,\"ZCR per section:\",zcr_per_section.shape,\n",
    "      \"SPC Per section: \",spectral_centroid_per_section.shape,\"Onsets Per section: \",onsets_per_section.shape,\n",
    "      \"AC global Per section: \",ac_global.shape\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_per_section = np.concatenate((mfcc_per_section,zcr_per_section,onsets_per_section,spectral_centroid_per_section,ac_global),axis =3)\n",
    "\n",
    "fps_song,fps_sect,fps_hops,fps_per_hop= features_per_section.shape\n",
    "\n",
    "features_per_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_per_section = np.reshape(features_per_section,(1,1,fps_hops*fps_per_hop))\n",
    "features_per_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complete_extract_features(song,num_of_sections,num_of_hops_per_section,num_of_mfcc,num_of_features,genre):\n",
    "    total_num_of_features = (num_of_features-1 + num_of_mfcc)* num_of_hops_per_section\n",
    "    song_length = len(song)\n",
    "    tot_num_of_hops = num_of_hops_per_section*num_of_sections\n",
    "    hop_width = int(np.floor(song_length/tot_num_of_hops))\n",
    "    section_width = hop_width*num_of_hops_per_section\n",
    "    song = song[:(hop_width*tot_num_of_hops)]\n",
    "    section_per_song = np.reshape(song,(1,num_of_sections,section_width))\n",
    "    sps_song,sps_sect,sps_width =  section_per_song.shape\n",
    "    \n",
    "    #print(section_per_song.shape)\n",
    "    try:\n",
    "        for i in range(sps_sect):\n",
    "            #print(i)\n",
    "            complete_n_fft = hop_width+1\n",
    "            one_song_section =section_per_song[0,i,:]\n",
    "            one_song_section = np.reshape(one_song_section,(1,1,one_song_section.shape[0]))\n",
    "            #print(one_song_section.shape)\n",
    "            hop_per_section = np.reshape(one_song_section,(1,1,num_of_hops_per_section,hop_width))\n",
    "            #print(hop_per_section.shape)\n",
    "            one_song_section_squeezed = np.squeeze(one_song_section)\n",
    "            #print(one_song_section_squeezed.shape)\n",
    "\n",
    "            mfcc_per_section = librosa.feature.mfcc(y=one_song_section_squeezed, sr=sampling_rate,\n",
    "                                        n_fft = complete_n_fft, n_mfcc=num_of_mfcc,hop_length = hop_width).T\n",
    "            mfcc_per_section = np.reshape(mfcc_per_section,(1,1,mfcc_per_section.shape[0],mfcc_per_section.shape[1]))\n",
    "            #print(\"mfcc\",mfcc_per_section.shape)\n",
    "            zcr_per_section = np.sum(librosa.core.zero_crossings(np.squeeze(hop_per_section)),axis =1 )\n",
    "            #print(zcr_per_section.shape)\n",
    "            zcr_per_section= np.reshape(zcr_per_section,(1,1,len(zcr_per_section),1))\n",
    "            #print(\"zcr\",zcr_per_section.shape)\n",
    "            onset_frames = librosa.onset.onset_detect(y=one_song_section_squeezed, sr=sampling_rate,units = 'samples')\n",
    "            song_splitter = np.arange(0,len(one_song_section_squeezed)+1, hop_width,dtype=int)\n",
    "            total_onsets = [0]\n",
    "            for j in range(1,len(song_splitter)):\n",
    "                total_onsets.append(np.sum(onset_frames<song_splitter[j]))\n",
    "            onsets_per_section = np.reshape(np.diff(total_onsets),(1,1,len(np.diff(total_onsets)),1))\n",
    "            #print(\"onset\",onsets_per_section.shape)\n",
    "            spectral_centroid_per_section = librosa.feature.spectral_centroid(y=one_song_section_squeezed, sr=sampling_rate, hop_length= hop_width,n_fft=complete_n_fft).T\n",
    "            spectral_centroid_per_section = np.reshape(spectral_centroid_per_section,(one_song_section.shape[0],one_song_section.shape[1],spectral_centroid_per_section.shape[0],spectral_centroid_per_section.shape[1]))\n",
    "            #print(\"spectral centroid\",spectral_centroid_per_section.shape)\n",
    "            #oenv = librosa.onset.onset_strength(y=one_song_section_squeezed, sr=sampling_rate, hop_length=hop_width, n_fft = (2*hop_width))\n",
    "            #tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sampling_rate,\n",
    "            #                              hop_length=hop_width)\n",
    "            #ac_global_temp = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n",
    "            #ac_global = np.reshape(ac_global_temp, (one_song_section.shape[0],one_song_section.shape[1],len(ac_global_temp),1))\n",
    "            #print(\"ac glob\",ac_global.shape)\n",
    "            feats_per_section = np.concatenate((mfcc_per_section,zcr_per_section,onsets_per_section,spectral_centroid_per_section),axis =3)\n",
    "            fps_song,fps_sect,fps_hops,fps_per_hop= feats_per_section.shape\n",
    "            #print(\"fps\",feats_per_section.shape)\n",
    "            feats_per_section = np.reshape(feats_per_section,(1,1,fps_hops*fps_per_hop))\n",
    "            feats_per_section= np.squeeze(feats_per_section,axis = 0)\n",
    "            #print()\n",
    "\n",
    "    except:\n",
    "        feats_per_section = np.zeros((1,total_num_of_features))\n",
    "        print(\"There was an error whilst computing the features\")\n",
    "    return feats_per_section\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_out = complete_extract_features(song,num_of_sections,num_of_hops_per_section,num_of_mfcc,num_of_features,10)\n",
    "\n",
    "test_out.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation of feature extraction for all the songs in the fma_small dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_num = 0\n",
    "tot_num_of_songs = cumulative_sum[-1]\n",
    "indiv_song_path= []\n",
    "for genre_path_name,genre_paths in paths_dict.items(): \n",
    "    song_num=song_num+1\n",
    "    indiv_song_path.append(genre_paths)\n",
    "  \n",
    "\n",
    "indiv_song_path_list = np.array(indiv_song_path).reshape(tot_num_of_songs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indiv_song_path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genres = []\n",
    "for song_num in range(len(indiv_song_path_list)):\n",
    "    temp = indiv_song_path_list[song_num].split(os.sep)[-2]\n",
    "    genres.append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoded_genres= le.fit(genres)\n",
    "encoded_genres\n",
    "\n",
    "#Label Encoding Mapping\n",
    "encoder_df= pd.DataFrame(data= {'Genre':genres,\n",
    "                   'Encoded_Genre':le.transform(genres)})\n",
    "encoder_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"I AM HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tot_num_of_songs = cumulative_sum[-1]\n",
    "final_d = np.zeros((tot_num_of_songs,test_out.shape[1]))\n",
    "print(final_d.shape)\n",
    "\n",
    "for song_num in range(len(indiv_song_path_list)):\n",
    "    try:\n",
    "        indiv_song_path = indiv_song_path_list[song_num]\n",
    "        song_signal = librosa.load(indiv_song_path,sr=None)[0]\n",
    "        curr_song_genre = encoder_df['Encoded_Genre'][song_num]\n",
    "        final_d[song_num]= complete_extract_features(song_signal,num_of_sections,num_of_hops_per_section,num_of_mfcc,num_of_features,curr_song_genre)\n",
    "                \n",
    "        print(complete_extract_features(song_signal,num_of_sections,num_of_hops_per_section,num_of_mfcc,num_of_features,curr_song_genre).shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "    except IOError as exc:\n",
    "        print(\"Unable to locate folder\")\n",
    "    \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"I AM HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "complete_df = pd.DataFrame(data=final_d.T,index =range(final_d.shape[1]))\n",
    "complete_df = complete_df.T\n",
    "\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "complete_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([complete_df,encoder_df],axis =1)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df.to_csv(\"FullSongFeatureExtractionFinallyDone.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
