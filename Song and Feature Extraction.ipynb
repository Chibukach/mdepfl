{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os,IPython, librosa, mir_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join,isdir\n",
    "from IPython.display import Audio\n",
    "from librosa.display import waveplot,specshow\n",
    "from librosa.onset import onset_strength, onset_detect\n",
    "from librosa.feature import melspectrogram, mfcc\n",
    "\n",
    "from collections import defaultdict,OrderedDict\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "import scipy\n",
    "from pandas import HDFStore,DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Extraction from fma_small\n",
    "\n",
    "1. Find the song folder path relative to the current computer\n",
    "2. Retrieve the different genre classifications\n",
    "3. Identify each song via its full path to song using index, to guarantee one-to-one mapping \n",
    "4. Sort in alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieving path to the fma_small directory and the corresponding meta data\n",
    "HOME_DIR = IPython.utils.path.get_home_dir()\n",
    "\n",
    "temp = join(HOME_DIR, 'Documents')\n",
    "path_to_small_fma = join(temp, 'fma_small')\n",
    "json_file = join(path_to_small_fma,'fma_small.json')\n",
    "#locate meta_dta\n",
    "print(path_to_small_fma)\n",
    "df = pd.read_json(json_file)\n",
    "print(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#only choose top genre as the label\n",
    "df = df.loc[:,['top_genre']]\n",
    "\n",
    "#ensure that the genre name matches file name in fma_small, \n",
    "#i.e Oldtime / Historian conflict issue\n",
    "df['top_genre']=df['top_genre'].apply(lambda y: y.split(os.sep)[0].strip())\n",
    "#locate each individual song by its full path\n",
    "df['temp'] = path_to_small_fma\n",
    "str_index = [\"%.2d\" % x for x in df.index]\n",
    "complete_genre_list = df['top_genre'].unique()\n",
    "le = LabelEncoder()\n",
    "encoded_genres= le.fit(complete_genre_list)\n",
    "print(complete_genre_list)\n",
    "\n",
    "#create full path to file and store as a single array\n",
    "df['full_path_to_song'] = df.temp.map(str)+ \"/\"+ df['top_genre'].values+ \"/\"+ str_index+ \".mp3\"\n",
    "del df['temp']\n",
    "#keep songs according to alphabetical order of songs \n",
    "df.sort_values(by = 'top_genre', inplace = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieve number of songs per genre\n",
    "genre_and_count = df['top_genre'].value_counts().sort_index()\n",
    "all_songs_path = df['full_path_to_song'].values  \n",
    "print(genre_and_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_counts = [] \n",
    "ordered_genres = [] \n",
    "full_song_df = OrderedDict()\n",
    "genre_to_song_dict = {}\n",
    "\n",
    "\n",
    "#retrieve number of songs per genre with the order preserved in two lists\n",
    "#ordered_genres\n",
    "#song_counts\n",
    "for i,genre in enumerate(genre_and_count.index):\n",
    "    ordered_genres.append(genre)\n",
    "    temp = df['top_genre'].value_counts()[genre]\n",
    "    song_counts.append(temp)\n",
    "\n",
    "genre_to_song_zipped = zip(ordered_genres,song_counts)\n",
    "\n",
    "num_of_genre = np.shape(genre_and_count)[0]\n",
    "print(\"The number of genres is:\", num_of_genre)\n",
    "\n",
    "#dictionary with each song count and its corresponding genre\n",
    "for genre,song_count in genre_to_song_zipped:\n",
    "    genre_to_song_dict[genre] = song_count\n",
    "\n",
    "genre_to_song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_genre = np.shape(genre_and_count)[0]\n",
    "all_songs_path = df['full_path_to_song'].values\n",
    "\n",
    "paths_dict = OrderedDict()\n",
    "\n",
    "#prepend zero so we have a start point for all_songs_path \n",
    "#and avoid messing with indices\n",
    "\n",
    "#use the cumulative sum to find none uniform ranges\n",
    "song_counts.insert(0,0)\n",
    "cumulative_sum = np.cumsum(song_counts,dtype=int)\n",
    "\n",
    "#creates a dictionary of the genres and its corresponding path\n",
    "for i,genre in enumerate(ordered_genres):\n",
    "    str1=genre\n",
    "    str2 = \"_paths\"\n",
    "    genre_paths = \"\".join((str1,str2))\n",
    "    paths_dict[genre_paths] = all_songs_path[cumulative_sum[i]:cumulative_sum[i+1]]\n",
    "\n",
    "#paths_dict\n",
    "#{genre_path_name: genre_paths}\n",
    "print(\"{'Electronic_paths:[array_of_all_electronic_paths]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assumed here that songs are Mono with sampling rate 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_of_songs = 3\n",
    "sampling_rate = 44100\n",
    "\n",
    "genre_signals_dict = OrderedDict()\n",
    "#creates a dictionary of the signals in a genre and their raw file\n",
    "for genre_path_name,genre_paths in paths_dict.items():\n",
    "    str1=genre_path_name[:-5]\n",
    "    str2 = \"signals\"\n",
    "    genre_signals = \"\".join((str1,str2))       \n",
    "    try:\n",
    "        first_three = genre_paths[:num_of_songs]\n",
    "        genre_signals_dict[genre_signals] = [\n",
    "        librosa.load(p,sr=sampling_rate,mono=True)[0] for p in first_three]\n",
    "    except IOError as exc:\n",
    "        print(\"Unable to locate folder\")\n",
    "        #raise IOError(\"%s: %s\" % (genre_paths, exc.strerror))\n",
    "        \n",
    "#genre_signals_dict\n",
    "#{genre_signals_name:genre_signals_paths}\n",
    "print(\"{'Electronic_signals:[array_of_all_electronic_paths]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the time series for each song according to the genres\n",
    "\n",
    "#sig_lengths = []\n",
    "for genre_signal_name,genre_signals in genre_signals_dict.items(): \n",
    "    for i, sig_amp in enumerate(genre_signals):\n",
    "        plt.subplot(1, num_of_songs, i+1)\n",
    "#        sig_lengths.append(len(sig_amp))\n",
    "        librosa.display.waveplot(sig_amp)\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.title(genre_signal_name)\n",
    "    plt.figure()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Work Flow\n",
    "\n",
    "## Segmentation - > Feature Extraction -> Classification\n",
    "\n",
    "\n",
    "1. Segmentation:\n",
    "2. Feature Extraction:\n",
    "3. Machine learning:\n",
    "\n",
    "https://ccrma.stanford.edu/wiki/MIR_workshop_2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Audio\n",
    "\n",
    "1. Time Domain Representation (Wave plot)\n",
    "2. Frequency Domain Representation (Spectogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampling_rate = 44100\n",
    "\n",
    "song_path = path_to_small_fma + \"/Hip-Hop/5.mp3\"\n",
    "song_path1 = path_to_small_fma + \"/Electronic/100538.mp3\"\n",
    "song_path2 = path_to_small_fma + \"/Pop/10396.mp3\"\n",
    "\n",
    "song_amp  = librosa.load(song_path, sampling_rate)[0]\n",
    "song_amp1  = librosa.load(song_path1, sampling_rate)[0]\n",
    "song_amp2  = librosa.load(song_path2, sampling_rate)[0]\n",
    "\n",
    "demo_songs =[song_amp,song_amp1,song_amp2]\n",
    "\n",
    "\n",
    "# Play it back!\n",
    "Audio(data=song_amp, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "waveplot(demo_songs[0],sampling_rate)\n",
    "plt.ylabel('Amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The zero crossing rate is the number of times the signal changes sign in a given period of time (usually one second).\n",
    "\n",
    "#### The zero crossing detector looks at the second derivative computed in discrete form calclate changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from librosa.feature import (zero_crossing_rate,spectral_centroid,\n",
    "spectral_bandwidth)\n",
    "from librosa import logamplitude\n",
    "\n",
    "sampling_rate = 44100\n",
    "\n",
    "man_features_list = []\n",
    "zcr = zero_crossing_rate(demo_songs[0],sampling_rate)\n",
    "plt.plot(zcr[0])\n",
    "plt.xlabel('Zero Crossing Rate')\n",
    "avg_zcr = np.mean(zcr)\n",
    "man_features_list.append(avg_zcr)\n",
    "man_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral features\n",
    "\n",
    "moments in stats i.e mean and variance, here we have anothers such as spectral centroid, bandwidth, skewness, kurtosis\n",
    "\n",
    "\n",
    "Spectral features are often used to analyse harmony or timbre. Usually the product of a spectogram and a filter bank\n",
    "\n",
    "## Pitch vs Pitch class\n",
    "Chroma measures the amount of energy in each pitch class,\n",
    "frame1, frame2,frame3 and collaspe down (many-to-one mapping) expectation \n",
    "of pitch class''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(demo_songs[0], sr=sampling_rate, n_fft=1024)\n",
    "logS = librosa.logamplitude(S)\n",
    "specshow(logS, sr=sampling_rate, x_axis='time', y_axis='mel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant-Q-transform\n",
    "\n",
    "#### constant q transform is for a direct log-frequency analysis in addition, one vertical move represents one semi-tone, so things are shit invariant vertically aswell as horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cqt = librosa.cqt(demo_songs[0],sampling_rate)\n",
    "chroma_cqt = librosa.feature.chroma_cqt(y = demo_songs[0],sr = sampling_rate)\n",
    "\n",
    "specshow(chroma_cqt)\n",
    "plt.title('chroma_cqt')\n",
    "plt.colorbar()\n",
    "\n",
    "specshow(librosa.logamplitude(cqt**2), x_axis = 'time', y_axis = 'cqt_hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fmin = librosa.midi_to_hz(36)\n",
    "C = librosa.cqt(demo_songs[0], sr=sampling_rate, fmin=fmin, n_bins=60)\n",
    "logC = librosa.logamplitude(C)\n",
    "librosa.display.specshow(logC, sr=sampling_rate, x_axis='time', y_axis='cqt_note', fmin=fmin, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(signal,sampling_rate,n_mfcc,genre):\n",
    "    from librosa.feature import (zero_crossing_rate,spectral_centroid,\n",
    "    spectral_bandwidth, tonnetz)\n",
    "\n",
    "    zcr = zero_crossing_rate(signal)[0]\n",
    "    norm_zcr = StandardScaler().fit_transform(zcr.reshape(1,-1))\n",
    "    avg_zcr = np.mean(zcr)\n",
    "    std_zcr = np.std(zcr)\n",
    "    \n",
    "    act_mfcc = mfcc(signal, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "    norm_mfcc = StandardScaler().fit_transform(act_mfcc)\n",
    "    avg_mfcc = np.mean(act_mfcc)\n",
    "    std_mfcc = np.std(act_mfcc)\n",
    "    \n",
    "#    act_tonnetz = tonnetz(signal, sr = sampling_rate)\n",
    "#    norm_tonnetz = StandardScaler().fit_transform(act_tonnetz)\n",
    "#    avg_tonnetz = np.mean(act_mfcc)\n",
    "#    std_tonnetz = np.std(act_mfcc)\n",
    "    \n",
    "    return [\n",
    "        avg_zcr,std_zcr,avg_mfcc,std_mfcc,le.transform([genre])[0]\n",
    "        #,avg_tonnetz,std_tonnetz\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_mfcc = 12\n",
    "d={}\n",
    "#'Average_tonnetz','Std_tonnetz'\n",
    "\n",
    "for i in range(len(demo_songs)):\n",
    "    song_num = \"song\"+ str(i)\n",
    "    if i ==0:\n",
    "        genre = \"Hip-Hop\"\n",
    "    elif i==1:\n",
    "        genre = \"Electronic\"\n",
    "    else:\n",
    "        genre = \"Pop\"\n",
    "        \n",
    "    d[song_num] = extract_features(demo_songs[i],sampling_rate,n_mfcc,genre)\n",
    "\n",
    "\n",
    "index = ['Average_zcr','Std_zcr','Average_mfcc','Std_mfcc','Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Label Encoding Mapping\n",
    "le_df= pd.DataFrame(data= {'Genre': complete_genre_list,\n",
    "                   'Encoded':le.transform(complete_genre_list)})\n",
    "le_df.to_csv('Encoder_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_df = pd.DataFrame(data=d,index =index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "demo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf = HDFStore('storage.h5')\n",
    "hdf.put('demoFile', demo_df, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "d={}\n",
    "song_num = 0\n",
    "for genre_path_name,genre_paths in paths_dict.items(): \n",
    "    song_num=song_num+1\n",
    "    #for i in range(len(all_songs_path)):\n",
    "    try:\n",
    "        for song_path in genre_paths:           \n",
    "            song_signal = librosa.load(song_path,sr=sampling_rate,mono=True)[0]\n",
    "            curr_song_genre= genre_path_name[:-6]\n",
    "            d[song_num]= extract_features(song_signal,sampling_rate,n_mfcc,curr_song_genre)\n",
    "    except IOError as exc:\n",
    "        print(\"Unable to locate folder\")\n",
    "            \n",
    "complete_df = pd.DataFrame(data=d,index =index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf = HDFStore('storage.h5')\n",
    "hdf.put('completeFile', complete_df, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
