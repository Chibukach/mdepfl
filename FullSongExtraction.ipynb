{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os,IPython, librosa, mir_eval\n",
    "from sys import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join,isdir\n",
    "from IPython.display import Audio\n",
    "from librosa.display import waveplot,specshow\n",
    "from librosa.onset import onset_strength, onset_detect\n",
    "from librosa.feature import melspectrogram, mfcc\n",
    "from librosa import load\n",
    "\n",
    "from collections import defaultdict,OrderedDict\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "import scipy\n",
    "from pandas import HDFStore,DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Extraction from fma_small\n",
    "\n",
    "1. Find the song folder path relative to the current computer\n",
    "2. Retrieve the different genre classifications\n",
    "3. Identify each song via its full path to song using index, to guarantee one-to-one mapping \n",
    "4. Sort in alphabetical order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieving path to the fma_small directory and the corresponding meta data\n",
    "HOME_DIR = IPython.utils.path.get_home_dir()\n",
    "\n",
    "temp = join(HOME_DIR, 'Documents')\n",
    "path_to_small_fma = join(temp, 'fma_small')\n",
    "json_file = join(path_to_small_fma,'fma_small.json')\n",
    "#locate meta_dta\n",
    "print(path_to_small_fma)\n",
    "df = pd.read_json(json_file)\n",
    "print(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#only choose top genre as the label\n",
    "df = df.loc[:,['top_genre']]\n",
    "\n",
    "#ensure that the genre name matches file name in fma_small, \n",
    "#i.e Oldtime / Historian conflict issue\n",
    "df['top_genre']=df['top_genre'].apply(lambda y: y.split(os.sep)[0].strip())\n",
    "#locate each individual song by its full path\n",
    "df['temp'] = path_to_small_fma\n",
    "str_index = [\"%.2d\" % x for x in df.index]\n",
    "complete_genre_list = df['top_genre'].unique()\n",
    "\n",
    "#create full path to file and store as a single array\n",
    "df['full_path_to_song'] = df.temp.map(str)+ \"/\"+ df['top_genre'].values+ \"/\"+ str_index+ \".mp3\"\n",
    "del df['temp']\n",
    "#keep songs according to alphabetical order of songs \n",
    "df.sort_values(by = 'top_genre', inplace = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "complete_genre_list_df = pd.DataFrame(complete_genre_list, columns = ['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieve number of songs per genre\n",
    "genre_and_count = df['top_genre'].value_counts().sort_index()\n",
    "all_songs_path = df['full_path_to_song'].values  \n",
    "print(genre_and_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_counts = [] \n",
    "ordered_genres = [] \n",
    "full_song_df = OrderedDict()\n",
    "genre_to_song_dict = {}\n",
    "\n",
    "\n",
    "#retrieve number of songs per genre with the order preserved in two lists\n",
    "#ordered_genres\n",
    "#song_counts\n",
    "for i,genre in enumerate(genre_and_count.index):\n",
    "    ordered_genres.append(genre)\n",
    "    temp = df['top_genre'].value_counts()[genre]\n",
    "    song_counts.append(temp)\n",
    "\n",
    "genre_to_song_zipped = zip(ordered_genres,song_counts)\n",
    "\n",
    "num_of_genre = np.shape(genre_and_count)[0]\n",
    "print(\"The number of genres is:\", num_of_genre)\n",
    "\n",
    "#dictionary with each song count and its corresponding genre\n",
    "for genre,song_count in genre_to_song_zipped:\n",
    "    genre_to_song_dict[genre] = song_count\n",
    "\n",
    "genre_to_song_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_genre = np.shape(genre_and_count)[0]\n",
    "all_songs_path = df['full_path_to_song'].values\n",
    "\n",
    "paths_dict = OrderedDict()\n",
    "\n",
    "#prepend zero so we have a start point for all_songs_path \n",
    "#and avoid messing with indices\n",
    "\n",
    "#use the cumulative sum to find none uniform ranges\n",
    "song_counts.insert(0,0)\n",
    "cumulative_sum = np.cumsum(song_counts,dtype=int)\n",
    "\n",
    "#creates a dictionary of the genres and its corresponding path\n",
    "for i,genre in enumerate(ordered_genres):\n",
    "    str1=genre\n",
    "    str2 = \"_paths\"\n",
    "    genre_paths = \"\".join((str1,str2))\n",
    "    paths_dict[genre_paths] = all_songs_path[cumulative_sum[i]:cumulative_sum[i+1]]\n",
    "\n",
    "#paths_dict\n",
    "#{genre_path_name: genre_paths}\n",
    "print(\"{'Electronic_paths:[array_of_all_electronic_paths]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_of_songs = 3\n",
    "sampling_rate = 44100\n",
    "\n",
    "genre_signals_dict = OrderedDict()\n",
    "#creates a dictionary of the signals in a genre and their raw file\n",
    "for genre_path_name,genre_paths in paths_dict.items():\n",
    "    str1=genre_path_name[:-5]\n",
    "    str2 = \"signals\"\n",
    "    genre_signals = \"\".join((str1,str2))       \n",
    "    try:\n",
    "        first_three = genre_paths[:num_of_songs]\n",
    "        genre_signals_dict[genre_signals] = [\n",
    "        load(p,sr=None)[0] for p in first_three]\n",
    "    except IOError as exc:\n",
    "        print(\"Unable to locate folder\")\n",
    "        #raise IOError(\"%s: %s\" % (genre_paths, exc.strerror))\n",
    "        \n",
    "#genre_signals_dict\n",
    "#{genre_signals_name:genre_signals_paths}\n",
    "print(\"{'Electronic_signals:[array_of_all_electronic_paths]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot the time series for each song according to the genres\n",
    "\n",
    "#sig_lengths = []\n",
    "for genre_signal_name,genre_signals in genre_signals_dict.items(): \n",
    "    for i, sig_amp in enumerate(genre_signals):\n",
    "        plt.subplot(1, num_of_songs, i+1)\n",
    "#        sig_lengths.append(len(sig_amp))\n",
    "        waveplot(sig_amp)\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.title(genre_signal_name)\n",
    "    plt.figure()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the complete feature extraction on a single song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#home laptop\n",
    "if \"Ubuntu\" in os.uname().version:\n",
    "    song, sampling_rate = librosa.load(\"/home/chib/Documents/fma_small/Electronic/99289.mp3\")\n",
    "\n",
    "#lab macbook\n",
    "elif \"Darwin\" in os.uname().version:\n",
    "    song, sampling_rate = librosa.load(\"/Users/chibmac/Documents/fma_small/Electronic/99289.mp3\")\n",
    "\n",
    "else: \n",
    "    song, sampling_rate = librosa.load(librosa.util.example_audio_file())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "song = 1\n",
    "\n",
    "\n",
    "full_info = np.zeros((1,1,1,1,1))\n",
    "full_info.shape\n",
    "\n",
    "np.zeros((5,5))\n",
    "song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_length = len(song)\n",
    "#number of chunks to split the song into\n",
    "num_of_hops_per_section = 7\n",
    "num_of_sections = 19\n",
    "\n",
    "tot_num_of_hops = num_of_hops_per_section*num_of_sections\n",
    "\n",
    "tot_num_of_hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4971"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hop_width = int(np.floor(song_length/tot_num_of_hops))\n",
    "hop_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34797"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_width = hop_width*num_of_hops_per_section\n",
    "section_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661143,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make song a multiple of the hops\n",
    "song = song[:(hop_width*tot_num_of_hops)]\n",
    "\n",
    "song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 34797)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_per_song = np.reshape(song,(num_of_sections,section_width))\n",
    "section_per_song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34797,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_song_section = section_per_song[0,:]\n",
    "one_song_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4971)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hop_per_section = np.reshape(one_song_section,(num_of_hops_per_section,hop_width))\n",
    "hop_per_section.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_mfcc = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_per_hop = librosa.feature.mfcc(y=one_song_section, sr=sampling_rate,\n",
    "                                    n_fft = section_width, n_mfcc=num_of_mfcc,hop_length = hop_width).T\n",
    "\n",
    "mfcc_per_hop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zcr_per_hop = np.sum(librosa.core.zero_crossings(hop_per_section),axis =1 )\n",
    "\n",
    "zcr_per_hop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind_unit_features=np.concatenate((unit_mfcc,np.array([unit_zcr]).T),axis=1)\n",
    "ind_unit_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_extract_features(song,num_of_sections,num_of_mfcc,genre):  \n",
    "    complete_split = num_of_sections *num_of_mfcc        \n",
    "    song_splitter = np.arange(0,song_length+1,song_length/num_of_sections,dtype=int)\n",
    "    genres = np.ones(num_of_sections,dtype=int) * genre\n",
    "    onset_frames = librosa.onset.onset_detect(y=song, sr=sampling_rate)\n",
    "    for i in range(len(song_splitter)):\n",
    "        unit_song = song[song_splitter[i]:song_splitter[i+1]]\n",
    "        unit_song_length = unit_song.shape[0]\n",
    "        window_width = np.int(unit_song_length/num_of_sections)+1\n",
    "        #unit_mfcc = librosa.feature.mfcc(y=unit_song, sr=sampling_rate,n_mfcc=num_of_mfcc,hop_length = window_width).T\n",
    "        \n",
    "        split = int(len(unit_song)/num_of_sections)*num_of_sections\n",
    "        unit_song= unit_song[:split]\n",
    "    \n",
    "        split_song = np.reshape(unit_song,(num_of_sections,int(unit_song_length/num_of_sections)))\n",
    "        unit_zcr = np.sum(librosa.core.zero_crossings(split_song),axis =1)\n",
    "        #ind_unit_features=np.concatenate((unit_mfcc,np.array([unit_zcr]).T),axis=1)\n",
    "        ind_unit_features=np.array([unit_zcr]).T\n",
    "        ind_unit_features = np.reshape(ind_unit_features,(1,np.size(ind_unit_features)))\n",
    "               \n",
    "        genre = np.matrix(genre)\n",
    "        \n",
    "        #rep_genre =np.array([np.ones(num_of_sections,dtype=int)* 24]).T \n",
    "        onset_frames = np.matrix(librosa.onset.onset_detect(y=unit_song, sr=sampling_rate))\n",
    "        #print(onset_frames.shape)\n",
    "        #print(genre.shape)\n",
    "        #print(num_of_sections)\n",
    "        #print(ind_unit_features.shape)\n",
    "        temp = np.concatenate((ind_unit_features,onset_frames),axis=1)\n",
    "\n",
    "        #print(full_feat_gen)\n",
    "        \n",
    "        cent = np.matrix(librosa.feature.spectral_centroid(y=unit_song, sr=sampling_rate, hop_length= window_width)[0]).mean(axis =1)\n",
    "        #print(cent.shape)\n",
    "        temp1 = np.concatenate((temp,cent),axis = 1)\n",
    "        oenv = librosa.onset.onset_strength(y=unit_song, sr=sampling_rate, hop_length=window_width)\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sampling_rate,\n",
    "                                      hop_length=window_width)\n",
    "        ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n",
    "        ac_global = np.matrix(librosa.util.normalize(ac_global))\n",
    "        #print (\"ok\",temp.shape)\n",
    "        #print(tempogram.shape)\n",
    "        #print(\"ok\",temp1.shape)\n",
    "        #print(\"on\",ac_global.shape)\n",
    "        temp2 = np.concatenate((temp1,ac_global),axis = 1)\n",
    "        full_feat_gen = np.concatenate((temp2,genre),axis=1)\n",
    "        #print(tempogram.shape)\n",
    "        return full_feat_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_out = complete_extract_features(song,num_of_sections,num_of_mfcc,10)\n",
    "\n",
    "test_out.shape\n",
    "test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation of feature extraction for all the songs in the fma_small dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_num = 0\n",
    "tot_num_of_songs = cumulative_sum[-1]\n",
    "indiv_song_path= []\n",
    "for genre_path_name,genre_paths in paths_dict.items(): \n",
    "    song_num=song_num+1\n",
    "    indiv_song_path.append(genre_paths)\n",
    "  \n",
    "\n",
    "indiv_song_path_list = np.array(indiv_song_path).reshape(tot_num_of_songs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indiv_song_path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genres = []\n",
    "for song_num in range(len(indiv_song_path_list)):\n",
    "    temp = indiv_song_path_list[song_num].split(os.sep)[-2]\n",
    "    genres.append(temp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoded_genres= le.fit(genres)\n",
    "encoded_genres\n",
    "\n",
    "#Label Encoding Mapping\n",
    "encoder_df= pd.DataFrame(data= {'Genre':genres,\n",
    "                   'Encoded_Genre':le.transform(genres)})\n",
    "encoder_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"I AM HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tot_num_of_songs = cumulative_sum[-1]\n",
    "final_d = np.zeros((tot_num_of_songs,test_out.shape[1]))\n",
    "print(final_d.shape)\n",
    "\n",
    "for song_num in range(len(indiv_song_path_list)):\n",
    "    try:\n",
    "        indiv_song_path = indiv_song_path_list[song_num]\n",
    "        song_signal = librosa.load(indiv_song_path,sr=None)[0]\n",
    "        curr_song_genre = encoder_df['Encoded_Genre'][song_num]\n",
    "        final_d[song_num]= complete_extract_features(song_signal,num_of_sections,num_of_mfcc,curr_song_genre)\n",
    "\n",
    "    except IOError as exc:\n",
    "        print(\"Unable to locate folder\")\n",
    "    \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"I AM HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "complete_df = pd.DataFrame(data=final_d.T,index =range(final_d.shape[1]))\n",
    "complete_df = complete_df.T\n",
    "\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "complete_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([complete_df,encoder_df],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([complete_df,encoder_df],axis =1)\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df.to_csv(\"complete_extract_with_onset.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
